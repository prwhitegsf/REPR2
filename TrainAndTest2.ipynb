{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import Layout, AppLayout\n",
    "from IPython.display import display\n",
    "from IPython.display import Audio\n",
    "from functools import partial\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import src.FeatureExtractors as fe\n",
    "from matplotlib.colors import Normalize\n",
    "import sklearn\n",
    "from sklearn.model_selection import GroupKFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "# get our meta data\n",
    "df = pd.read_csv('datasets/RAVDESS/metadata/RAVDESS.csv',usecols=['actor','actor_sex','emotion','label','filepath'])\n",
    "\n",
    "# Choose the feature set(s) and add to dataframe\n",
    "\n",
    "def choose_features(mfcc=40, mel=128):\n",
    "    scaler = StandardScaler()\n",
    "    #features = scaler.fit_transform(features)\n",
    "    # mfcc only\n",
    "    if mfcc != 'None' and mel == 'None':\n",
    "        return scaler.fit_transform(np.load(f'datasets/RAVDESS/features/mfcc/mfcc{mfcc}.npy'))\n",
    "    # mels only\n",
    "    elif mfcc == 'None' and mel != 'None':\n",
    "        return scaler.fit_transform(np.load(f'datasets/RAVDESS/features/mel/mel{mel}.npy'))\n",
    "    # both\n",
    "    elif mfcc != 'None' and mel != 'None':\n",
    "        mfcc_frame = np.load(f'datasets/RAVDESS/features/mfcc/mfcc{mfcc}.npy')\n",
    "        mel_frame = np.load(f'datasets/RAVDESS/features/mel/mel{mel}.npy')\n",
    "        feature_matrix=np.array([])\n",
    "        feature_matrix = np.hstack((mfcc_frame, mel_frame))\n",
    "        return scaler.fit_transform(feature_matrix)\n",
    "\n",
    "\n",
    "\n",
    "df['features'] = list(choose_features(40,128))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# split data set functions\n",
    "def random_split(df,test_size=0.2):\n",
    "    labels = np.array(df['label'])\n",
    "    features = np.vstack(df['features'])\n",
    "    \n",
    "    return train_test_split(features, labels,test_size=test_size)\n",
    "\n",
    "\n",
    "def split_by_actor_sex(df,train_set='male'):\n",
    "    train = df\n",
    "    test = df\n",
    "    if train_set == 'male':\n",
    "        train = df[df['actor_sex'] == 'male']\n",
    "        test = df[df['actor_sex'] == 'female']\n",
    "    elif train_set == 'female':\n",
    "        train = df[df['actor_sex'] == 'female']\n",
    "        test = df[df['actor_sex'] == 'male']\n",
    "\n",
    "    # , np.vstack(test['features']),np.array(train['label'],np.array(test['label']))\n",
    "    return np.vstack(train['features']), np.vstack(test['features']), np.array(train['label']),np.array(test['label'])\n",
    "\n",
    "\n",
    "# CV scoring function -- if I end up wanting it, which maybe I don't?\n",
    "sklearn.set_config(enable_metadata_routing=True)\n",
    "def get_model_cv_scores(model, features, labels):\n",
    "    cv = GroupKFold(5)\n",
    "    rng = np.random.RandomState(7)\n",
    "    groups = rng.randint(0, 10, size=len(labels))\n",
    "    scoring = ['recall_micro', 'precision_micro','f1_micro','balanced_accuracy','roc_auc']\n",
    "    scores = cross_validate(model, features, labels,scoring=scoring,cv=cv, params={\"groups\":groups})\n",
    "    return pd.DataFrame(scores)\n",
    "\n",
    "# Split the dataset into training and tests -- \n",
    "feature_train,feature_test,label_train,label_test = random_split(df, 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV took 10.66 seconds for 28 candidate parameter settings.\n"
     ]
    }
   ],
   "source": [
    "# Grid Search\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import recall_score, balanced_accuracy_score\n",
    "param_grid={\n",
    "    \"C\" : [0.1,0.5,0.8,1.0,1.2,1.5,2],\n",
    "    \"gamma\" : [\"auto\",\"scale\"],\n",
    "    \"class_weight\" : [\"balanced\",None]\n",
    "}\n",
    "\n",
    "\n",
    "from time import time\n",
    "cv = GroupKFold(5)\n",
    "rng = np.random.RandomState(7)\n",
    "groups = rng.randint(0, 10, size=len(label_train))\n",
    "grid_search = GridSearchCV(SVC(kernel='rbf'), param_grid=param_grid,scoring='recall',cv=cv,return_train_score=True)\n",
    "start = time()\n",
    "grid_search.fit(feature_train, label_train, groups=groups)\n",
    "\n",
    "print(\n",
    "    \"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n",
    "    % (time() - start, len(grid_search.cv_results_[\"params\"]))\n",
    ")\n",
    "use_cols = ['param_C','param_class_weight',  'param_gamma', 'mean_test_score','mean_train_score','std_test_score', 'rank_test_score']\n",
    "gr = pd.DataFrame(grid_search.cv_results_)[use_cols]\n",
    "gr.sort_values(by='rank_test_score',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x2a41c41ad50>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay,RocCurveDisplay,PrecisionRecallDisplay,DetCurveDisplay\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import recall_score,precision_score,f1_score,hinge_loss,accuracy_score,d2_absolute_error_score\n",
    "\n",
    "\n",
    "class ResultsViewer:\n",
    "\n",
    "    def __init__(self, df, features_train, labels_train,features_test, labels_test):\n",
    "        \n",
    "        self.df = self.arrange_columns(df)\n",
    "        \n",
    "        self.model = SVC()\n",
    "        self.current_record_idx = 0\n",
    "\n",
    "        self.features = features_train\n",
    "        self.labels = labels_train\n",
    "\n",
    "        self.features_train = features_train\n",
    "        self.labels_train = labels_train\n",
    "\n",
    "        # Testing only\n",
    "        self.features_test = features_test\n",
    "        self.labels_test = labels_test\n",
    "        self.test_model = None\n",
    "        self.test_mode = 0\n",
    "\n",
    "        self.predictions = []\n",
    "\n",
    "\n",
    "    def arrange_columns(self, in_df):\n",
    "        df = in_df.copy()\n",
    "        df['rank']=list(range(1,len(df)+1))\n",
    "        df.drop(columns=['rank_test_score'],inplace=True)\n",
    "        df.columns=['L2Mult','weight','gamma','test_recall','train_recall', 'test_stdev','rank']\n",
    "        df = df[['rank','L2Mult','weight','gamma','test_recall','train_recall', 'test_stdev']]\n",
    "        return df\n",
    "    \n",
    "\n",
    "    def get_top_records(self, num_records=10):\n",
    "        return self.df.iloc[0:num_records]\n",
    "\n",
    "    def format_top_records_table(self, recs,num_records=10):\n",
    "\n",
    "        cols = ['rank', 'weight','gamma','L2Mult','test recall', 'train recall']\n",
    "\n",
    "        cell_text = []\n",
    "        for i in range(num_records):\n",
    "            \n",
    "            weight = 'bal'\n",
    "            if recs['weight'].iloc[i] == None:\n",
    "                weight =\"unbal\"\n",
    "            \n",
    "            cell_text.append([recs['rank'].iloc[i], \n",
    "                              weight,\n",
    "                              recs['gamma'].iloc[i],\n",
    "                              round(recs['L2Mult'].iloc[i],7),\n",
    "                              round(recs['test_recall'].iloc[i],2),\n",
    "                              round(recs['train_recall'].iloc[i],2)])\n",
    "        return cols, cell_text\n",
    "\n",
    "    def print_records_to_table(self):\n",
    "       \n",
    "        cols, cell_text = self.format_top_records_table(self.get_top_records())\n",
    "\n",
    "        fig,ax = plt.subplots(figsize=(3,3),layout=\"constrained\")\n",
    "        fig.suptitle(\"Top Grid Search Results\")\n",
    "        \n",
    "        fig.patch.set_visible(False)\n",
    "        ax.axis('off')\n",
    "        ax.axis('tight')\n",
    "        fig.canvas.header_visible = False\n",
    "        \n",
    "        col_widths = [0.15,0.16,0.17,0.16,0.18,0.18]\n",
    "\n",
    "        table = ax.table(cellText=cell_text,colLabels=cols,loc='center',colWidths=col_widths)\n",
    "        table.auto_set_font_size(False)\n",
    "        table.set_fontsize(8)\n",
    "        table.scale(1.5,1.5)\n",
    "        plt.close()\n",
    "        return fig\n",
    "    \n",
    "  \n",
    "  \n",
    "  ####################################################################################\n",
    "    \n",
    "    def select_record(self,idx):\n",
    "        plt.close()\n",
    "        self.current_record_idx = idx\n",
    "        self.fit_model_with_record(idx)\n",
    "\n",
    "    \n",
    "    def apply_record_to_model(self, rec_index=0):\n",
    "        \n",
    "        return SVC(C=self.df['L2Mult'].iloc[rec_index],\n",
    "                   kernel='rbf',\n",
    "                   gamma=self.df['gamma'].iloc[rec_index],\n",
    "                   class_weight=self.df['weight'].iloc[rec_index]\n",
    "                   )\n",
    "     \n",
    "           \n",
    "\n",
    "\n",
    "    def fit_model_with_record(self, rec_index=0):    \n",
    "        self.model = self.apply_record_to_model(rec_index=rec_index)\n",
    "        self.model = self.model.fit(self.features,self.labels)\n",
    "        return self.model\n",
    "       \n",
    "    def set_testing_model(self):\n",
    "        self.test_model = self.model\n",
    "        self.test_mode = 1\n",
    "        self.predictions = self.test_model.predict(self.features_test)\n",
    "\n",
    "\n",
    "\n",
    "    def show_confusion_matrix_train(self):\n",
    "        fig,axs = plt.subplots(figsize=(3,3), layout=\"constrained\")\n",
    "        axs.set_title(\"Confusion Matrix Train\")\n",
    "        ConfusionMatrixDisplay.from_estimator(self.model, self.features_train, self.labels_train,ax=axs,colorbar=False)\n",
    "        plt.close()\n",
    "        return fig\n",
    "\n",
    "    def show_confusion_matrix_test(self):\n",
    "        fig,axs = plt.subplots(figsize=(3,3), layout=\"constrained\")\n",
    "        axs.set_title(\"Confusion Matrix Test\")\n",
    "        if self.test_mode == 1:\n",
    "            ConfusionMatrixDisplay.from_predictions(self.labels_test,self.predictions,ax=axs,colorbar=False,cmap=\"magma\")\n",
    "        plt.close()\n",
    "        return fig\n",
    "\n",
    "    def show_ROC(self):\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(4,4),layout=\"constrained\")\n",
    "        fig.canvas.header_visible = False\n",
    "        ax.set_title(\"ROC Curve\")\n",
    "        RocCurveDisplay.from_estimator(self.model, self.features_train, self.labels_train,ax=ax,name=\"ROC Training\") \n",
    "        \n",
    "        if self.test_mode == 1:\n",
    "            pred = self.test_model.decision_function(self.features_test)\n",
    "            RocCurveDisplay.from_predictions(self.labels_test,pred,ax=ax,name=\"ROC Test\")\n",
    "        \n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.close()\n",
    "        return fig\n",
    "\n",
    "\n",
    "    def show_DET(self):\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(4,3),layout=\"constrained\")\n",
    "        ax.set_title(\"DET Curve\")\n",
    "        fig.canvas.header_visible = False\n",
    "        DetCurveDisplay.from_estimator(self.model, self.features_train, self.labels_train,ax=ax)\n",
    "        if self.test_mode == 1:\n",
    "            pred = self.test_model.decision_function(self.features_test)\n",
    "            DetCurveDisplay.from_predictions(self.labels_test,pred,ax=ax)\n",
    "        plt.ylabel('False Negative Rate')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.close()\n",
    "        return fig\n",
    "\n",
    "    def show_precision_recall(self):\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(4,3),layout=\"constrained\")\n",
    "        fig.canvas.header_visible = False\n",
    "        ax.set_title(\"Precision Recall Curve (PRC)\")\n",
    "        PrecisionRecallDisplay.from_estimator(self.model, self.features_train, self.labels_train,ax=ax,name=\"PRC Train\",plot_chance_level=True) \n",
    "        \n",
    "        if self.test_mode == 1:\n",
    "            pred = self.test_model.decision_function(self.features_test)\n",
    "            PrecisionRecallDisplay.from_predictions(self.labels_test,pred,ax=ax,name=\"PRC Test\",plot_chance_level=True)\n",
    "        \n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.close()\n",
    "        return fig\n",
    "\n",
    "    def get_train_metrics(self):\n",
    "        # obtain scores\n",
    "        \n",
    "        scoring = ['recall','precision','accuracy','d2_absolute_error_score']\n",
    "        scores = cross_validate(self.model, self.features, self.labels,scoring=scoring)\n",
    "        \n",
    "        cols = ['recall','precision','accuracy','d2_error']\n",
    "        cells = []\n",
    "        cells.append(round(scores['test_recall'].mean(),2))\n",
    "        cells.append(round(scores['test_precision'].mean(),2))\n",
    "        \n",
    "        cells.append(round(scores['test_accuracy'].mean(),2))\n",
    "        #cells.append(round(scores['test_f1_micro'].mean(),2))\n",
    "        cells.append(round(scores['test_d2_absolute_error_score'].mean(),2))\n",
    "\n",
    "        return cols, cells\n",
    "\n",
    "\n",
    "    def get_test_metrics(self):\n",
    "        pred = self.test_model.decision_function(self.features_test)\n",
    "        cols = ['recall','recall_micro','precision','d2_error']\n",
    "        cells = []\n",
    "        cells.append(round(recall_score(self.labels_test, self.predictions),3))\n",
    "        cells.append(round(precision_score(self.labels_test, self.predictions),3))\n",
    "        cells.append(round(accuracy_score(self.labels_test, self.predictions),3))\n",
    "        cells.append(round(d2_absolute_error_score(self.labels_test,self.predictions),3))\n",
    "        \n",
    "        return cols, cells\n",
    "    \n",
    "    def show_train_metrics(self):\n",
    "        \n",
    "        cols, cell_text = self.get_train_metrics()\n",
    "        rows = [\"Train Results\"]\n",
    "        fig,axs = plt.subplots(figsize=(2,2))\n",
    "        fig.suptitle(\"Model Performance\")\n",
    "        fig.canvas.header_visible = False\n",
    "        fig.patch.set_visible(False)\n",
    "        axs.axis('off')\n",
    "        axs.axis('tight')\n",
    "        \n",
    "        table = axs.table( colLabels=cols,rowLabels=rows,cellText=[cell_text],loc='center')\n",
    "        table.auto_set_font_size(False)\n",
    "        table.set_fontsize(8)\n",
    "        table.scale(1.5,1.5)\n",
    "        plt.close()\n",
    "        return fig\n",
    "\n",
    "    def show_test_metrics(self):\n",
    "        cols , train_row = self.get_train_metrics()\n",
    "        _, test_row = self.get_test_metrics()\n",
    "        \n",
    "        \n",
    "        rows = [\"Training\",\"Testing\"]\n",
    "        fig,axs = plt.subplots(figsize=(3,2))\n",
    "        fig.suptitle(\"Model Performance\")\n",
    "        fig.canvas.header_visible = False\n",
    "        fig.patch.set_visible(False)\n",
    "        axs.axis('off')\n",
    "        axs.axis('tight')\n",
    "        \n",
    "        table = axs.table( colLabels=cols,rowLabels=rows,cellText=[train_row, test_row],loc='center')\n",
    "        table.auto_set_font_size(False)\n",
    "        table.set_fontsize(8)\n",
    "        table.scale(1.5,1.5)\n",
    "        plt.close()\n",
    "        return fig\n",
    "\n",
    "\n",
    "\n",
    "# populate the training results records\n",
    "tr = ResultsViewer(gr,feature_train, label_train,feature_test,label_test)\n",
    "tr.fit_model_with_record(0)\n",
    "# prevent plot from displpaying automatically\n",
    "plt.ioff() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No such comm: 2ebb2d91640c4900857dbfe240eb022b\n",
      "No such comm: 2ebb2d91640c4900857dbfe240eb022b\n",
      "No such comm: 2ebb2d91640c4900857dbfe240eb022b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No such comm: dff09d9e143d4d7eaa3dccf258683dbf\n",
      "No such comm: dff09d9e143d4d7eaa3dccf258683dbf\n",
      "No such comm: dff09d9e143d4d7eaa3dccf258683dbf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No such comm: d719e0a6fb1f41c59f47f5f469ac0aee\n",
      "No such comm: d719e0a6fb1f41c59f47f5f469ac0aee\n",
      "No such comm: d719e0a6fb1f41c59f47f5f469ac0aee\n"
     ]
    }
   ],
   "source": [
    "rec_sel_label = widgets.Label(value=\"Select Record to View\")\n",
    "\n",
    "rec_sel_dropdown = widgets.Dropdown(\n",
    "    options=['1','2','3','4','5','6','7','8','9','10'],\n",
    "    value='1',\n",
    "    description='',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "\n",
    "# Handlers\n",
    "params_out = widgets.Output()\n",
    "record_out = widgets.Output()\n",
    "roc_out = widgets.Output()\n",
    "det_out = widgets.Output()\n",
    "stats_out =  widgets.Output()\n",
    "cm_out = widgets.Output()\n",
    "cm2_out = widgets.Output()\n",
    "prec_recall_out = widgets.Output()\n",
    "test_metrics_out = widgets.Output()\n",
    "\n",
    "run_oos_button  = widgets.Button(description='Test model on out of sample data')\n",
    "\n",
    "record_box = widgets.VBox([rec_sel_label,rec_sel_dropdown,record_out,prec_recall_out,run_oos_button])\n",
    "\n",
    "model_box = widgets.VBox([test_metrics_out,prec_recall_out])\n",
    "stat_box = widgets.VBox([stats_out,cm_out,cm2_out])\n",
    "chart_box = widgets.VBox([roc_out, det_out])\n",
    "left_box = record_box\n",
    "\n",
    "\n",
    "def initialize():\n",
    "    \n",
    "    #with params_out:\n",
    "     #   params_out.clear_output()\n",
    "      #  display(tr.show_curr_record_params())\n",
    "\n",
    "    with record_out:\n",
    "\n",
    "        record_out.clear_output()\n",
    "        display(tr.print_records_to_table())\n",
    "\n",
    "    with stats_out:\n",
    "        stats_out.clear_output()\n",
    "        display(tr.show_train_metrics())\n",
    "\n",
    "    with cm2_out:\n",
    "        cm2_out.clear_output()\n",
    "\n",
    "    with cm_out:\n",
    "        cm_out.clear_output()\n",
    "        display(tr.show_confusion_matrix_train())\n",
    "\n",
    "    with roc_out:\n",
    "        roc_out.clear_output()\n",
    "        display(tr.show_ROC())\n",
    "\n",
    "    with det_out:\n",
    "        det_out.clear_output()\n",
    "        display(tr.show_DET())\n",
    "    \n",
    "    \n",
    "\n",
    "    with prec_recall_out:\n",
    "        prec_recall_out.clear_output()\n",
    "        display(tr.show_precision_recall())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80385c4d17ab4bc3a11b145a279f82f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(Label(value='Select Record to View'), Dropdown(options=('1', '2', '3', '4', '5',…"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def select_record_to_view(dfx,names):\n",
    "    left_box = record_box\n",
    "    val = int(names.new) - 1\n",
    "    dfx.select_record(val)\n",
    "  \n",
    "   # with params_out:\n",
    "    #    params_out.clear_output()\n",
    "     #   display(tr.show_curr_record_params())\n",
    "    \n",
    "    with stats_out:\n",
    "        stats_out.clear_output()\n",
    "        display(dfx.show_train_metrics())\n",
    "\n",
    "    with cm_out:\n",
    "        cm_out.clear_output()\n",
    "        display(dfx.show_confusion_matrix_train())\n",
    "\n",
    "    with roc_out:\n",
    "        roc_out.clear_output()\n",
    "        display(dfx.show_ROC())\n",
    "\n",
    "    with det_out:\n",
    "        det_out.clear_output()\n",
    "        display(dfx.show_DET())\n",
    "\n",
    "    with prec_recall_out:\n",
    "        prec_recall_out.clear_output()\n",
    "        display(dfx.show_precision_recall())\n",
    "\n",
    "    plt.close('all')\n",
    "    \n",
    "\n",
    "def test_model_on_oos(dfx,val):\n",
    "    \n",
    "    dfx.set_testing_model()\n",
    "   # with params_out:\n",
    "    #    params_out.clear_output()\n",
    "     #   display(tr.show_curr_record_params())\n",
    "\n",
    "    with stats_out:\n",
    "        stats_out.clear_output()\n",
    "        display(dfx.show_test_metrics())\n",
    "\n",
    "    with cm_out:\n",
    "        cm_out.clear_output()\n",
    "        display(dfx.show_confusion_matrix_train())\n",
    "\n",
    "    with roc_out:\n",
    "        roc_out.clear_output()\n",
    "        display(dfx.show_ROC())\n",
    "\n",
    "    with det_out:\n",
    "        det_out.clear_output()\n",
    "        display(dfx.show_DET())\n",
    "    \n",
    "    with prec_recall_out:\n",
    "        prec_recall_out.clear_output()\n",
    "        display(dfx.show_precision_recall())\n",
    "    \n",
    "    with cm2_out:\n",
    "        cm2_out.clear_output()\n",
    "        display(dfx.show_confusion_matrix_test())\n",
    "\n",
    "        \n",
    "\n",
    "    with test_metrics_out:\n",
    "        test_metrics_out.clear_output()\n",
    "        \n",
    "\n",
    "\n",
    "    plt.close('all')     \n",
    "\n",
    "\n",
    "initialize()\n",
    "\n",
    "\n",
    "\n",
    "rec_sel_dropdown.observe(partial(select_record_to_view,tr),names='value')\n",
    "run_oos_button.on_click(partial(test_model_on_oos, tr))\n",
    "\n",
    "gridsearch_panel = widgets.HBox([left_box, stat_box,chart_box])\n",
    "\n",
    "gridsearch_panel"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
